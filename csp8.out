model, <class 'transformer.Models.Transformer'>
Replaced Linear
encoder, <class 'transformer.Models.Encoder'>
src_word_emb, <class 'torch.nn.modules.sparse.Embedding'>
position_enc, <class 'transformer.Models.PositionalEncoding'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_stack, <class 'torch.nn.modules.container.ModuleList'>
0, <class 'transformer.Layers.EncoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
1, <class 'transformer.Layers.EncoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
2, <class 'transformer.Layers.EncoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
3, <class 'transformer.Layers.EncoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
4, <class 'transformer.Layers.EncoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
5, <class 'transformer.Layers.EncoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
decoder, <class 'transformer.Models.Decoder'>
trg_word_emb, <class 'torch.nn.modules.sparse.Embedding'>
position_enc, <class 'transformer.Models.PositionalEncoding'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_stack, <class 'torch.nn.modules.container.ModuleList'>
0, <class 'transformer.Layers.DecoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
enc_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
1, <class 'transformer.Layers.DecoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
enc_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
2, <class 'transformer.Layers.DecoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
enc_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
3, <class 'transformer.Layers.DecoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
enc_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
4, <class 'transformer.Layers.DecoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
enc_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
5, <class 'transformer.Layers.DecoderLayer'>
slf_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
enc_attn, <class 'transformer.SubLayers.MultiHeadAttention'>
Replaced Linear
Replaced Linear
Replaced Linear
Replaced Linear
w_qs, <class 'CSP.pruned_layers.PrunedLinear'>
w_ks, <class 'CSP.pruned_layers.PrunedLinear'>
w_vs, <class 'CSP.pruned_layers.PrunedLinear'>
fc, <class 'CSP.pruned_layers.PrunedLinear'>
attention, <class 'transformer.Modules.ScaledDotProductAttention'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
pos_ffn, <class 'transformer.SubLayers.PositionwiseFeedForward'>
Replaced Linear
Replaced Linear
w_1, <class 'CSP.pruned_layers.PrunedLinear'>
w_2, <class 'CSP.pruned_layers.PrunedLinear'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
dropout, <class 'torch.nn.modules.dropout.Dropout'>
layer_norm, <class 'torch.nn.modules.normalization.LayerNorm'>
trg_word_prj, <class 'CSP.pruned_layers.PrunedLinear'>
[Info] Trained model state loaded.
Layer id	Type		Parameter	Non-zero parameter	Sparsity(\%)
1		PLinear		262144		25512			0.902679
2		PLinear		262144		21056			0.919678
3		PLinear		262144		39760			0.848328
4		PLinear		262144		120296			0.541107
5		PLinear		1048576		119904			0.885651
6		PLinear		1048576		60960			0.941864
7		PLinear		262144		23984			0.908508
8		PLinear		262144		24224			0.907593
9		PLinear		262144		35664			0.863953
10		PLinear		262144		63832			0.756500
11		PLinear		1048576		148240			0.858627
12		PLinear		1048576		74872			0.928596
13		PLinear		262144		20952			0.920074
14		PLinear		262144		21272			0.918854
15		PLinear		262144		36504			0.860748
16		PLinear		262144		40800			0.844360
17		PLinear		1048576		160368			0.847061
18		PLinear		1048576		67384			0.935738
19		PLinear		262144		18224			0.930481
20		PLinear		262144		18128			0.930847
21		PLinear		262144		35240			0.865570
22		PLinear		262144		37128			0.858368
23		PLinear		1048576		197944			0.811226
24		PLinear		1048576		76448			0.927094
25		PLinear		262144		25352			0.903290
26		PLinear		262144		24824			0.905304
27		PLinear		262144		35168			0.865845
28		PLinear		262144		35304			0.865326
29		PLinear		1048576		201856			0.807495
30		PLinear		1048576		85920			0.918060
31		PLinear		262144		19352			0.926178
32		PLinear		262144		20416			0.922119
33		PLinear		262144		38216			0.854218
34		PLinear		262144		37192			0.858124
35		PLinear		1048576		216984			0.793068
36		PLinear		1048576		72992			0.930389
37		PLinear		262144		35888			0.863098
38		PLinear		262144		35320			0.865265
39		PLinear		262144		68952			0.736969
40		PLinear		262144		93392			0.643738
41		PLinear		262144		19328			0.926270
42		PLinear		262144		19480			0.925690
43		PLinear		262144		54312			0.792816
44		PLinear		262144		61296			0.766174
45		PLinear		1048576		307064			0.707161
46		PLinear		1048576		204400			0.805069
47		PLinear		262144		38272			0.854004
48		PLinear		262144		37656			0.856354
49		PLinear		262144		63696			0.757019
50		PLinear		262144		61040			0.767151
51		PLinear		262144		28816			0.890076
52		PLinear		262144		28080			0.892883
53		PLinear		262144		41984			0.839844
54		PLinear		262144		44992			0.828369
55		PLinear		1048576		270400			0.742126
56		PLinear		1048576		186656			0.821991
57		PLinear		262144		44360			0.830780
58		PLinear		262144		42688			0.837158
59		PLinear		262144		53528			0.795807
60		PLinear		262144		60176			0.770447
61		PLinear		262144		30056			0.885345
62		PLinear		262144		29472			0.887573
63		PLinear		262144		35312			0.865295
64		PLinear		262144		41768			0.840668
65		PLinear		1048576		233000			0.777794
66		PLinear		1048576		173328			0.834702
67		PLinear		262144		36384			0.861206
68		PLinear		262144		37776			0.855896
69		PLinear		262144		58672			0.776184
70		PLinear		262144		62120			0.763031
71		PLinear		262144		30040			0.885406
72		PLinear		262144		29056			0.889160
73		PLinear		262144		36552			0.860565
74		PLinear		262144		42344			0.838470
75		PLinear		1048576		244216			0.767097
76		PLinear		1048576		182872			0.825600
77		PLinear		262144		18016			0.931274
78		PLinear		262144		18360			0.929962
79		PLinear		262144		50920			0.805756
80		PLinear		262144		52144			0.801086
81		PLinear		262144		31248			0.880798
82		PLinear		262144		30096			0.885193
83		PLinear		262144		37944			0.855255
84		PLinear		262144		42544			0.837708
85		PLinear		1048576		249608			0.761955
86		PLinear		1048576		171496			0.836449
87		PLinear		262144		22400			0.914551
88		PLinear		262144		22816			0.912964
89		PLinear		262144		50104			0.808868
90		PLinear		262144		47944			0.817108
91		PLinear		262144		31144			0.881195
92		PLinear		262144		29336			0.888092
93		PLinear		262144		37192			0.858124
94		PLinear		262144		40640			0.844971
95		PLinear		1048576		303504			0.710556
96		PLinear		1048576		179720			0.828606
97		PLinear		4874752		3061616			0.371944
PrunedLinear:totalLinear ratio = 1.0
Total nonzero parameters: 10035808
Total parameters: 48914944
Total sparsity: 0.794831
[Info] Finished.
0.2937086342789103
